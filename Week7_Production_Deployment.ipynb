{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c08dd4c",
   "metadata": {},
   "source": [
    "# Week 7: Production Deployment\n",
    "\n",
    "## Learning Objectives\n",
    "- Learn best practices for deploying LLMs in production\n",
    "- Understand model serving, API design, and load balancing\n",
    "- Implement monitoring, logging, and error handling\n",
    "- Explore scaling strategies and cost optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0cdbde",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction to Production Deployment](#introduction)\n",
    "2. [Model Serving](#model-serving)\n",
    "3. [API Design & Load Balancing](#api-design)\n",
    "4. [Monitoring & Logging](#monitoring-logging)\n",
    "5. [Error Handling & Reliability](#error-handling)\n",
    "6. [Scaling Strategies](#scaling)\n",
    "7. [Cost Optimization](#cost-optimization)\n",
    "8. [Hands-on Project](#hands-on-project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b92c7b5",
   "metadata": {},
   "source": [
    "## 1. Introduction to Production Deployment <a id='introduction'></a>\n",
    "\n",
    "### What is Production Deployment?\n",
    "Production deployment is the process of making your LLM-based application available to real users. This involves moving from a development environment to a scalable, reliable, and secure infrastructure.\n",
    "\n",
    "- Why production deployment is different from prototyping\n",
    "- Key challenges: scalability, reliability, security, cost\n",
    "- Overview of deployment options: cloud, on-premises, hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd366a5",
   "metadata": {},
   "source": [
    "## 2. Model Serving <a id='model-serving'></a>\n",
    "\n",
    "### Model Serving: Approaches and Tools\n",
    "- What is model serving?\n",
    "- Batch vs. real-time inference\n",
    "- Model server options: FastAPI, Flask, TorchServe, Triton Inference Server, Hugging Face Inference Endpoints\n",
    "- Containerization: Docker basics for ML models\n",
    "- Example: Serving a Hugging Face model with FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Serve a Hugging Face model with FastAPI\n",
    "from fastapi import FastAPI\n",
    "from transformers import pipeline\n",
    "\n",
    "app = FastAPI()\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(text: str):\n",
    "    return classifier(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283c3549",
   "metadata": {},
   "source": [
    "## 3. API Design & Load Balancing <a id='api-design'></a>\n",
    "\n",
    "### API Design & Load Balancing\n",
    "- REST vs. gRPC for ML APIs\n",
    "- Designing robust endpoints (input validation, error handling)\n",
    "- Load balancing strategies: round-robin, least connections, cloud-native solutions (AWS ELB, Azure Load Balancer)\n",
    "- Example: API versioning and health checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a868c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Add a health check endpoint to FastAPI\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    return {\"status\": \"ok\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f6eeef",
   "metadata": {},
   "source": [
    "## 4. Monitoring & Logging <a id='monitoring-logging'></a>\n",
    "\n",
    "### Monitoring & Logging\n",
    "- Importance of monitoring in production\n",
    "- Metrics to track: latency, throughput, error rate, resource usage\n",
    "- Logging best practices (structured logs, log aggregation)\n",
    "- Tools: Prometheus, Grafana, ELK stack, cloud monitoring solutions\n",
    "- Example: Adding logging to FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6d8de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "@app.post(\"/predict-logged\")\n",
    "def predict_logged(text: str):\n",
    "    logging.info(f\"Prediction requested for: {text}\")\n",
    "    return classifier(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4870b00",
   "metadata": {},
   "source": [
    "## 5. Error Handling & Reliability <a id='error-handling'></a>\n",
    "\n",
    "### Error Handling & Reliability\n",
    "- Common failure modes in ML systems\n",
    "- Graceful degradation and fallback strategies\n",
    "- Circuit breakers, retries, and timeouts\n",
    "- Example: Handling exceptions in FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eed792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import HTTPException\n",
    "\n",
    "@app.post(\"/predict-safe\")\n",
    "def predict_safe(text: str):\n",
    "    try:\n",
    "        return classifier(text)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during prediction: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Model inference failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b378b78",
   "metadata": {},
   "source": [
    "## 6. Scaling Strategies <a id='scaling'></a>\n",
    "\n",
    "### Scaling Strategies\n",
    "- Vertical vs. horizontal scaling\n",
    "- Auto-scaling in the cloud (Kubernetes, AWS ECS, Azure AKS)\n",
    "- Model sharding and multi-model serving\n",
    "- Caching for performance (Redis, CDN)\n",
    "- Example: Docker Compose for scaling services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9d4d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Docker Compose YAML for scaling FastAPI service\n",
    "# (This is a YAML snippet, not Python code)\n",
    "# version: '3'\n",
    "# services:\n",
    "#   app:\n",
    "#     build: .\n",
    "#     ports:\n",
    "#       - \"8000:8000\"\n",
    "#     deploy:\n",
    "#       replicas: 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f211126b",
   "metadata": {},
   "source": [
    "## 7. Cost Optimization <a id='cost-optimization'></a>\n",
    "\n",
    "### Cost Optimization\n",
    "- Monitoring and controlling cloud costs\n",
    "- Model quantization and distillation for cheaper inference\n",
    "- Spot instances and serverless options\n",
    "- Example: Using ONNX for efficient inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33da2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Export a Hugging Face model to ONNX for optimized inference\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "dummy_input = torch.zeros(1, 8, dtype=torch.long)\n",
    "torch.onnx.export(model, (dummy_input,), \"model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6a3376",
   "metadata": {},
   "source": [
    "## 8. Hands-on Project <a id='hands-on-project'></a>\n",
    "\n",
    "### Hands-on Project: Open-Source Production Deployment of a Scalable LLM API for Marketing, Sales, and Finance\n",
    "\n",
    "This project guides you through a real-world, open-source style deployment of an LLM-powered API for business use cases. You'll use widely adopted open-source tools and best practices for reliability, scalability, and maintainability.\n",
    "\n",
    "#### Step 1: Design Your API\n",
    "- Define endpoints for:\n",
    "  - `/generate-marketing`: Generate product descriptions, ad copy, etc.\n",
    "  - `/sales-support`: Lead qualification, automated Q&A\n",
    "  - `/financial-analysis`: Summarize reports, extract trends\n",
    "- Specify input/output schemas using OpenAPI (FastAPI auto-generates docs)\n",
    "\n",
    "#### Step 2: Build the Service\n",
    "- Use FastAPI for the web server and API documentation\n",
    "- Use Hugging Face Transformers for LLM inference\n",
    "- Structure your codebase with clear separation (routers, services, utils)\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from transformers import pipeline\n",
    "\n",
    "app = FastAPI()\n",
    "marketing_gen = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "\n",
    "@app.post(\"/generate-marketing\")\n",
    "def generate_marketing(prompt: str):\n",
    "    return marketing_gen(prompt)\n",
    "# Repeat for /sales-support and /financial-analysis\n",
    "```\n",
    "\n",
    "#### Step 3: Add Monitoring, Logging, and Error Handling\n",
    "- Integrate Prometheus for metrics (use [prometheus_fastapi_instrumentator](https://github.com/trallard/prometheus-fastapi-instrumentator))\n",
    "- Use Python's `logging` module for structured logs\n",
    "- Add exception handlers for robust error reporting\n",
    "\n",
    "#### Step 4: Containerize with Docker\n",
    "- Write a `Dockerfile` for reproducible builds\n",
    "- Use multi-stage builds for smaller images\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.10-slim\n",
    "WORKDIR /app\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "COPY . .\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```\n",
    "\n",
    "#### Step 5: Orchestrate with Docker Compose or Kubernetes\n",
    "- Use `docker-compose.yml` to run multiple replicas and a reverse proxy (e.g., Traefik or Nginx)\n",
    "- For advanced scaling, use Kubernetes (Helm charts, Horizontal Pod Autoscaler)\n",
    "\n",
    "#### Step 6: Load Balancing and Auto-Scaling\n",
    "- Use Nginx/Traefik for HTTP load balancing in Docker Compose\n",
    "- In Kubernetes, configure a Service and HPA for auto-scaling\n",
    "\n",
    "#### Step 7: Cost Optimization\n",
    "- Quantize models with Hugging Face Optimum or ONNX Runtime\n",
    "- Use batch inference endpoints for high-throughput scenarios\n",
    "- Monitor resource usage with Grafana dashboards\n",
    "\n",
    "#### Step 8: Open-Source Best Practices\n",
    "- Add a `README.md` with API usage examples\n",
    "- Use `.env` files for secrets/configuration (never hardcode credentials)\n",
    "- Write unit/integration tests (e.g., with pytest and httpx)\n",
    "- Set up CI/CD (e.g., GitHub Actions) for automated builds and tests\n",
    "\n",
    "#### Step 9: Document and Share\n",
    "- Publish your code on GitHub with an open-source license\n",
    "- Include deployment instructions and sample API requests\n",
    "- Encourage community contributions and feedback\n",
    "\n",
    "---\n",
    "\n",
    "**References & Further Reading:**\n",
    "- [FastAPI Production Deployment Guide](https://fastapi.tiangolo.com/deployment/)\n",
    "- [Dockerizing FastAPI](https://testdriven.io/blog/fastapi-docker-traefik/)\n",
    "- [Prometheus FastAPI Instrumentator](https://github.com/trallard/prometheus-fastapi-instrumentator)\n",
    "- [Hugging Face Optimum](https://huggingface.co/docs/optimum/intel/usage_guides/quantization)\n",
    "- [Kubernetes for ML](https://github.com/kubeflow/kubeflow)\n",
    "- [Open Source LLM API Example](https://github.com/huggingface/text-generation-inference)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
