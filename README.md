# LLM Course: From Fundamentals to Advanced Applications

A comprehensive hands-on course covering Large Language Models (LLMs) from basic NLP concepts to building advanced search engines and retrieval systems.

## üìö Course Structure

### Week 1: Introduction to NLP
- **File**: `Week1_Introduction_to_NLP.ipynb`
- **Topics**: 
  - Text preprocessing and tokenization
  - Word embeddings (Word2Vec, GloVe)
  - Basic language models (N-grams, RNNs)
  - Practical implementations with real datasets

### Week 2: Transformers and LLM System Design
- **File**: `Week2_Transformers_and_LLM_System_Design.ipynb`
- **Topics**:
  - Attention mechanisms
  - Transformer architecture deep dive
  - Encoder-decoder designs
  - Scaling and optimization strategies

### Week 3: Semantic Search and Retrieval
- **File**: `Week3_Semantic_Search_and_Retrieval.ipynb`
- **Topics**:
  - Vector embeddings and similarity metrics
  - Vector databases (FAISS, ChromaDB)
  - RAG (Retrieval-Augmented Generation) architecture
  - Advanced retrieval techniques

### Week 4: Building Search Engine from Scratch
- **File**: `Week4_Building_Search_Engine_from_Scratch.ipynb`
- **Topics**:
  - End-to-end search engine implementation
  - Indexing and query processing
  - Ranking algorithms
  - Performance optimization

## üöÄ Getting Started

### Prerequisites
- Python 3.8+
- Jupyter Notebook or JupyterLab
- Basic understanding of machine learning concepts

### Installation

1. Clone this repository:
```bash
git clone <your-repo-url>
cd llm-course
```

2. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install required packages:
```bash
pip install -r requirements.txt
```

4. Launch Jupyter:
```bash
jupyter notebook
```

## üìã Requirements

See `requirements.txt` for a complete list of dependencies. Key packages include:
- `torch` - PyTorch for deep learning
- `transformers` - Hugging Face transformers library
- `numpy` - Numerical computing
- `pandas` - Data manipulation
- `scikit-learn` - Machine learning utilities
- `matplotlib` & `seaborn` - Data visualization
- `faiss-cpu` - Vector similarity search
- `chromadb` - Vector database
- `datasets` - Dataset loading utilities

## üéØ Learning Objectives

By the end of this course, you will be able to:
- Understand the fundamental concepts of NLP and language modeling
- Implement transformer architectures from scratch
- Build semantic search systems using vector embeddings
- Create end-to-end search engines with ranking algorithms
- Apply RAG techniques for enhanced information retrieval
- Optimize LLM systems for production use

## üîß Course Features

- **Hands-on Implementation**: Every concept is accompanied by practical code examples
- **Real Datasets**: Work with actual text corpora and datasets
- **Progressive Complexity**: Each week builds upon previous knowledge
- **Production-Ready Code**: Learn industry best practices and optimization techniques
- **Interactive Notebooks**: Explore concepts through Jupyter notebooks with visualizations

## üìñ Additional Resources

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformer paper
- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)
- [Hugging Face Documentation](https://huggingface.co/docs)

## ü§ù Contributing

Feel free to contribute to this course by:
- Adding new examples or exercises
- Improving existing notebooks
- Fixing bugs or typos
- Suggesting new topics or improvements

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

## üë®‚Äçüíª Author

Created with ‚ù§Ô∏è for the LLM learning community.

---

**Happy Learning! üöÄ**
