{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b4bcc88",
   "metadata": {},
   "source": [
    "# Week 2: Transformers and LLM System Design\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the evolution of Transformer models\n",
    "- Learn about encoder-decoder architectures\n",
    "- Explore attention mechanisms\n",
    "- Design LLM systems and understand their components\n",
    "- Implement basic transformer components\n",
    "\n",
    "## Table of Contents\n",
    "1. [Evolution of Language Models](#evolution-of-language-models)\n",
    "2. [Attention Mechanism](#attention-mechanism)\n",
    "3. [Transformer Architecture](#transformer-architecture)\n",
    "4. [Encoder-Decoder Design](#encoder-decoder-design)\n",
    "5. [LLM System Components](#llm-system-components)\n",
    "6. [Implementation Examples](#implementation-examples)\n",
    "7. [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe720d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch transformers datasets tokenizers matplotlib seaborn numpy pandas\n",
    "!pip install torch-audio torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbf173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,\n",
    "    BertModel, GPT2Model, T5Model, pipeline\n",
    ")\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e42192e",
   "metadata": {},
   "source": [
    "## Evolution of Language Models\n",
    "\n",
    "Let's explore the evolution from simple n-gram models to modern Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a49569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeline of language model evolution\n",
    "evolution_timeline = {\n",
    "    \"N-gram Models (1990s)\": \"Statistical models based on word sequences\",\n",
    "    \"Neural Language Models (2003)\": \"Feed-forward neural networks for language modeling\",\n",
    "    \"RNNs/LSTMs (2010s)\": \"Recurrent networks for sequential data\",\n",
    "    \"Seq2Seq (2014)\": \"Encoder-decoder architecture with RNNs\",\n",
    "    \"Attention (2015)\": \"Attention mechanism for better long-range dependencies\",\n",
    "    \"Transformer (2017)\": \"Self-attention based architecture\",\n",
    "    \"BERT (2018)\": \"Bidirectional encoder representations\",\n",
    "    \"GPT (2018-present)\": \"Generative pre-trained transformers\",\n",
    "    \"T5 (2019)\": \"Text-to-text transfer transformer\",\n",
    "    \"Large Models (2020+)\": \"GPT-3, PaLM, ChatGPT, GPT-4\"\n",
    "}\n",
    "\n",
    "for year, description in evolution_timeline.items():\n",
    "    print(f\"{year}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db05038",
   "metadata": {},
   "source": [
    "## Attention Mechanism\n",
    "\n",
    "The attention mechanism allows models to focus on relevant parts of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025ad3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_q = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.W_k = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.W_v = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # Compute attention scores\n",
    "        Q = self.W_q(query)  # (batch_size, seq_len, hidden_size)\n",
    "        K = self.W_k(key)    # (batch_size, seq_len, hidden_size)\n",
    "        V = self.W_v(value)  # (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.hidden_size)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores.masked_fill_(mask == 0, -1e9)\n",
    "            \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attended_values = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return attended_values, attention_weights\n",
    "\n",
    "# Example usage\n",
    "hidden_size = 64\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "attention = SimpleAttention(hidden_size)\n",
    "x = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "output, weights = attention(x, x, x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04734136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights\n",
    "def visualize_attention(attention_weights, tokens=None):\n",
    "    \"\"\"Visualize attention weights as a heatmap\"\"\"\n",
    "    # Take the first sample from the batch\n",
    "    weights = attention_weights[0].detach().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(weights, annot=True, cmap='Blues', fmt='.2f')\n",
    "    plt.title('Attention Weights Heatmap')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    \n",
    "    if tokens:\n",
    "        plt.xticks(range(len(tokens)), tokens, rotation=45)\n",
    "        plt.yticks(range(len(tokens)), tokens, rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the attention weights from our example\n",
    "visualize_attention(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f54c22",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "Multi-head attention allows the model to attend to information from different representation subspaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b90945",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores.masked_fill_(mask == 0, -1e9)\n",
    "            \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attended_values = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return attended_values, attention_weights\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size, seq_len, d_model = query.size()\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Apply attention\n",
    "        attended_values, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attended_values = attended_values.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, d_model\n",
    "        )\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.W_o(attended_values)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Example usage\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "seq_len = 20\n",
    "batch_size = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, weights = mha(x, x, x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3facee",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "\n",
    "Let's implement a simplified Transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a6e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output, _ = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "seq_len = 20\n",
    "batch_size = 2\n",
    "\n",
    "transformer_block = TransformerBlock(d_model, num_heads, d_ff)\n",
    "pos_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "x_with_pos = pos_encoding(x)\n",
    "output = transformer_block(x_with_pos)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896107bc",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Design\n",
    "\n",
    "Understanding different architectural patterns in Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72096275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different model architectures\n",
    "model_architectures = {\n",
    "    \"BERT\": {\n",
    "        \"type\": \"Encoder-only\",\n",
    "        \"use_case\": \"Understanding tasks (classification, NER, QA)\",\n",
    "        \"attention\": \"Bidirectional\",\n",
    "        \"training\": \"Masked Language Modeling\"\n",
    "    },\n",
    "    \"GPT\": {\n",
    "        \"type\": \"Decoder-only\",\n",
    "        \"use_case\": \"Generation tasks (text completion, dialogue)\",\n",
    "        \"attention\": \"Causal (left-to-right)\",\n",
    "        \"training\": \"Autoregressive Language Modeling\"\n",
    "    },\n",
    "    \"T5\": {\n",
    "        \"type\": \"Encoder-Decoder\",\n",
    "        \"use_case\": \"Text-to-text tasks (translation, summarization)\",\n",
    "        \"attention\": \"Bidirectional encoder + Causal decoder\",\n",
    "        \"training\": \"Span-based denoising\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for model, details in model_architectures.items():\n",
    "    print(f\"\\n{model}:\")\n",
    "    for key, value in details.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba96fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and compare different pre-trained models\n",
    "def compare_models():\n",
    "    # BERT model\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    bert_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # GPT-2 model\n",
    "    gpt2_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "    gpt2_model = GPT2Model.from_pretrained('gpt2')\n",
    "    \n",
    "    # Add padding token for GPT-2\n",
    "    gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "    \n",
    "    text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "    \n",
    "    # BERT encoding\n",
    "    bert_inputs = bert_tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        bert_outputs = bert_model(**bert_inputs)\n",
    "    \n",
    "    # GPT-2 encoding\n",
    "    gpt2_inputs = gpt2_tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        gpt2_outputs = gpt2_model(**gpt2_inputs)\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"\\nBERT:\")\n",
    "    print(f\"  Input tokens: {bert_tokenizer.convert_ids_to_tokens(bert_inputs['input_ids'][0])}\")\n",
    "    print(f\"  Output shape: {bert_outputs.last_hidden_state.shape}\")\n",
    "    print(f\"  Model size: {sum(p.numel() for p in bert_model.parameters()):,} parameters\")\n",
    "    \n",
    "    print(f\"\\nGPT-2:\")\n",
    "    print(f\"  Input tokens: {gpt2_tokenizer.convert_ids_to_tokens(gpt2_inputs['input_ids'][0])}\")\n",
    "    print(f\"  Output shape: {gpt2_outputs.last_hidden_state.shape}\")\n",
    "    print(f\"  Model size: {sum(p.numel() for p in gpt2_model.parameters()):,} parameters\")\n",
    "\n",
    "compare_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d0b4fa",
   "metadata": {},
   "source": [
    "## LLM System Components\n",
    "\n",
    "Understanding the key components of LLM systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76075c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMSystemComponents:\n",
    "    \"\"\"\n",
    "    A class to demonstrate the key components of an LLM system\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.components = {\n",
    "            \"Tokenization\": {\n",
    "                \"purpose\": \"Convert text to tokens\",\n",
    "                \"types\": [\"Byte-Pair Encoding (BPE)\", \"SentencePiece\", \"WordPiece\"],\n",
    "                \"considerations\": [\"Vocabulary size\", \"OOV handling\", \"Subword tokenization\"]\n",
    "            },\n",
    "            \"Model Architecture\": {\n",
    "                \"purpose\": \"Process and transform inputs\",\n",
    "                \"types\": [\"Encoder-only\", \"Decoder-only\", \"Encoder-Decoder\"],\n",
    "                \"considerations\": [\"Model size\", \"Context length\", \"Attention patterns\"]\n",
    "            },\n",
    "            \"Training Strategy\": {\n",
    "                \"purpose\": \"Learn from data\",\n",
    "                \"types\": [\"Pre-training\", \"Fine-tuning\", \"RLHF\"],\n",
    "                \"considerations\": [\"Data quality\", \"Compute resources\", \"Training stability\"]\n",
    "            },\n",
    "            \"Inference Engine\": {\n",
    "                \"purpose\": \"Generate outputs efficiently\",\n",
    "                \"types\": [\"Autoregressive\", \"Parallel\", \"Speculative decoding\"],\n",
    "                \"considerations\": [\"Latency\", \"Throughput\", \"Memory usage\"]\n",
    "            },\n",
    "            \"Deployment Infrastructure\": {\n",
    "                \"purpose\": \"Serve the model at scale\",\n",
    "                \"types\": [\"Cloud-based\", \"Edge deployment\", \"Hybrid\"],\n",
    "                \"considerations\": [\"Scalability\", \"Cost\", \"Security\"]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def describe_component(self, component_name):\n",
    "        if component_name in self.components:\n",
    "            comp = self.components[component_name]\n",
    "            print(f\"\\n{component_name}:\")\n",
    "            print(f\"  Purpose: {comp['purpose']}\")\n",
    "            print(f\"  Types: {', '.join(comp['types'])}\")\n",
    "            print(f\"  Key Considerations: {', '.join(comp['considerations'])}\")\n",
    "        else:\n",
    "            print(f\"Component '{component_name}' not found.\")\n",
    "    \n",
    "    def list_all_components(self):\n",
    "        print(\"LLM System Components:\")\n",
    "        for component in self.components.keys():\n",
    "            self.describe_component(component)\n",
    "\n",
    "# Example usage\n",
    "llm_system = LLMSystemComponents()\n",
    "llm_system.list_all_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048980fb",
   "metadata": {},
   "source": [
    "## Implementation Examples\n",
    "\n",
    "Let's implement some practical examples using pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ecc6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text classification with BERT\n",
    "def text_classification_example():\n",
    "    # Load pre-trained model\n",
    "    model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    classifier = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "    \n",
    "    # Test texts\n",
    "    texts = [\n",
    "        \"I love this new technology!\",\n",
    "        \"This is terrible and disappointing.\",\n",
    "        \"The weather is okay today.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Text Classification Results:\")\n",
    "    for text in texts:\n",
    "        result = classifier(text)\n",
    "        print(f\"Text: '{text}'\")\n",
    "        print(f\"Sentiment: {result[0]['label']} (confidence: {result[0]['score']:.3f})\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "text_classification_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e7abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation with GPT-2\n",
    "def text_generation_example():\n",
    "    generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "    \n",
    "    prompt = \"The future of artificial intelligence is\"\n",
    "    \n",
    "    # Generate text with different parameters\n",
    "    results = generator(\n",
    "        prompt,\n",
    "        max_length=100,\n",
    "        num_return_sequences=3,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=generator.tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(\"\\nGenerated texts:\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. {result['generated_text']}\")\n",
    "\n",
    "text_generation_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bb8579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Answering with BERT\n",
    "def question_answering_example():\n",
    "    qa_pipeline = pipeline(\"question-answering\")\n",
    "    \n",
    "    context = \"\"\"\n",
    "    The Transformer is a deep learning model introduced in 2017, used primarily in the field of \n",
    "    natural language processing (NLP). Like recurrent neural networks (RNNs), Transformers are \n",
    "    designed to handle sequential input data, such as natural language, for tasks such as \n",
    "    translation and text summarization. However, unlike RNNs, Transformers do not require that \n",
    "    the sequential data be processed in order. Since Transformers can process data in parallel, \n",
    "    they are much faster to train than RNNs.\n",
    "    \"\"\"\n",
    "    \n",
    "    questions = [\n",
    "        \"When was the Transformer introduced?\",\n",
    "        \"What field primarily uses Transformers?\",\n",
    "        \"Why are Transformers faster to train than RNNs?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Question Answering Results:\")\n",
    "    for question in questions:\n",
    "        result = qa_pipeline(question=question, context=context)\n",
    "        print(f\"Q: {question}\")\n",
    "        print(f\"A: {result['answer']} (confidence: {result['score']:.3f})\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "question_answering_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c999df1c",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Implement a Simple Transformer for Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb85f88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, num_classes, max_len=512):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_model * 4)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Embedding and positional encoding\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer(x, mask)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Example usage\n",
    "vocab_size = 10000\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "num_classes = 3\n",
    "seq_len = 50\n",
    "batch_size = 4\n",
    "\n",
    "model = SimpleTransformerClassifier(vocab_size, d_model, num_heads, num_layers, num_classes)\n",
    "sample_input = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "output = model(sample_input)\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565bd06a",
   "metadata": {},
   "source": [
    "### Exercise 2: Compare Model Behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3c8f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_behaviors():\n",
    "    \"\"\"Compare how different models handle the same input\"\"\"\n",
    "    \n",
    "    text = \"The cat sat on the [MASK].\"\n",
    "    \n",
    "    # BERT for masked language modeling\n",
    "    bert_pipeline = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "    bert_results = bert_pipeline(text)\n",
    "    \n",
    "    print(\"BERT Masked Language Modeling:\")\n",
    "    print(f\"Input: {text}\")\n",
    "    for result in bert_results[:3]:\n",
    "        print(f\"  {result['sequence']} (score: {result['score']:.3f})\")\n",
    "    \n",
    "    # GPT-2 for text generation\n",
    "    gpt2_pipeline = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "    prompt = \"The cat sat on the\"\n",
    "    gpt2_results = gpt2_pipeline(\n",
    "        prompt, \n",
    "        max_length=len(prompt.split()) + 5,\n",
    "        num_return_sequences=3,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=gpt2_pipeline.tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nGPT-2 Text Generation:\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    for result in gpt2_results:\n",
    "        print(f\"  {result['generated_text']}\")\n",
    "\n",
    "compare_model_behaviors()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5a3361",
   "metadata": {},
   "source": [
    "### Exercise 3: Analyze Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf6930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attention_patterns():\n",
    "    \"\"\"Analyze attention patterns in a pre-trained BERT model\"\"\"\n",
    "    from transformers import BertTokenizer, BertModel\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
    "    \n",
    "    text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get attention weights from the last layer, first head\n",
    "    attention = outputs.attentions[-1][0, 0].numpy()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Attention shape: {attention.shape}\")\n",
    "    \n",
    "    # Visualize attention\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        attention, \n",
    "        xticklabels=tokens, \n",
    "        yticklabels=tokens, \n",
    "        annot=True, \n",
    "        fmt='.2f',\n",
    "        cmap='Blues'\n",
    "    )\n",
    "    plt.title('BERT Attention Weights (Last Layer, Head 1)')\n",
    "    plt.xlabel('Key Tokens')\n",
    "    plt.ylabel('Query Tokens')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "analyze_attention_patterns()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e8b47a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this module, we covered:\n",
    "- Evolution from RNNs to Transformers\n",
    "- Attention mechanisms and multi-head attention\n",
    "- Transformer architecture components\n",
    "- Different model designs (encoder-only, decoder-only, encoder-decoder)\n",
    "- LLM system components and considerations\n",
    "- Practical implementations and examples\n",
    "\n",
    "## Next Steps\n",
    "In the next module, we'll explore semantic search and retrieval systems, building on the transformer foundations we've established here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b9d0aa",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "- [Attention Is All You Need (Original Transformer Paper)](https://arxiv.org/abs/1706.03762)\n",
    "- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)\n",
    "- [Language Models are Unsupervised Multitask Learners (GPT-2)](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers/index)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
