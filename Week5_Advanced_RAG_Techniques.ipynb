{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "920d4f56",
   "metadata": {},
   "source": [
    "# Week 5: Advanced RAG Techniques\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand multi-modal RAG and graph-based retrieval\n",
    "- Explore knowledge graphs and their integration with LLMs\n",
    "- Apply advanced prompt engineering and chain-of-thought reasoning\n",
    "- Implement memory systems and context management for conversational AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f867ad91",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction to Advanced RAG](#introduction)\n",
    "2. [Multi-modal RAG](#multi-modal-rag)\n",
    "3. [Graph-based Retrieval](#graph-based-retrieval)\n",
    "4. [Knowledge Graphs](#knowledge-graphs)\n",
    "5. [Advanced Prompt Engineering](#advanced-prompt-engineering)\n",
    "6. [Chain-of-Thought Reasoning](#chain-of-thought-reasoning)\n",
    "7. [Memory Systems & Context Management](#memory-systems)\n",
    "8. [Hands-on Project](#hands-on-project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342a1dd2",
   "metadata": {},
   "source": [
    "## 1. Introduction to Advanced RAG <a id='introduction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12795c2e",
   "metadata": {},
   "source": [
    "### What is Advanced RAG?\n",
    "Retrieval-Augmented Generation (RAG) combines retrieval systems with generative models to enhance factuality and context. Advanced RAG extends this by integrating multi-modal data, graphs, and advanced reasoning.\n",
    "\n",
    "- Why RAG? Limitations of pure generation and retrieval\n",
    "- Key components: retriever, generator, fusion\n",
    "- Advanced use cases: multi-modal, graph, conversational"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e1afa2",
   "metadata": {},
   "source": [
    "## 2. Multi-modal RAG <a id='multi-modal-rag'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea63a98",
   "metadata": {},
   "source": [
    "### Multi-modal RAG\n",
    "Multi-modal RAG incorporates text, images, audio, and other data types into the retrieval and generation process.\n",
    "\n",
    "- Use cases: document Q&A with images, video search, audio transcripts\n",
    "- Model architectures: CLIP, BLIP, multi-modal transformers\n",
    "- Example: Using CLIP for image-text retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561babc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Retrieve images relevant to a text query using CLIP\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "images = [Image.open(\"example1.jpg\"), Image.open(\"example2.jpg\")]\n",
    "text = [\"A photo of a cat\"]\n",
    "\n",
    "inputs = processor(text=text, images=images, return_tensors=\"pt\", padding=True)\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image\n",
    "probs = logits_per_image.softmax(dim=1)\n",
    "print(\"Probabilities:\", probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8766b0a3",
   "metadata": {},
   "source": [
    "## 3. Graph-based Retrieval <a id='graph-based-retrieval'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf4676f",
   "metadata": {},
   "source": [
    "### Graph-based Retrieval\n",
    "Graph-based retrieval leverages knowledge graphs or semantic graphs to enhance retrieval quality and reasoning.\n",
    "\n",
    "- What is a knowledge graph? Nodes, edges, relations\n",
    "- Graph traversal for context expansion\n",
    "- Example: Using NetworkX for simple graph-based retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63edcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple graph traversal with NetworkX\n",
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "G.add_edges_from([(\"Paris\", \"France\"), (\"France\", \"Europe\"), (\"Paris\", \"Eiffel Tower\")])\n",
    "print(\"Neighbors of Paris:\", list(G.neighbors(\"Paris\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b507f2ee",
   "metadata": {},
   "source": [
    "## 4. Knowledge Graphs <a id='knowledge-graphs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022e622a",
   "metadata": {},
   "source": [
    "### Knowledge Graphs\n",
    "Knowledge graphs structure information for better retrieval and reasoning.\n",
    "\n",
    "- Building a knowledge graph from text (spaCy, Stanford OpenIE)\n",
    "- Integrating knowledge graphs with LLMs\n",
    "- Example: Extracting entities and relations from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9614f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Entity and relation extraction with spaCy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Barack Obama was born in Hawaii.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "for token in doc:\n",
    "    if token.dep_ == \"ROOT\":\n",
    "        print(f\"Relation: {token.head.text} -> {token.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b177ef",
   "metadata": {},
   "source": [
    "## 5. Advanced Prompt Engineering <a id='advanced-prompt-engineering'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356db73f",
   "metadata": {},
   "source": [
    "### Advanced Prompt Engineering\n",
    "Prompt engineering is crucial for controlling LLM behavior in RAG systems.\n",
    "\n",
    "- Prompt templates for retrieval-augmented tasks\n",
    "- Few-shot and chain-of-thought prompting\n",
    "- Example: Prompt template for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c1a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Prompt template for RAG\n",
    "prompt = \"\"\"You are a helpful assistant. Use the following context to answer the question.\\nContext: {context}\\nQuestion: {question}\\nAnswer:\"\"\"\n",
    "context = \"The Eiffel Tower is in Paris.\"\n",
    "question = \"Where is the Eiffel Tower?\"\n",
    "print(prompt.format(context=context, question=question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5849c766",
   "metadata": {},
   "source": [
    "## 6. Chain-of-Thought Reasoning <a id='chain-of-thought-reasoning'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedf361c",
   "metadata": {},
   "source": [
    "### Chain-of-Thought Reasoning\n",
    "Chain-of-thought (CoT) prompting encourages LLMs to reason step by step.\n",
    "\n",
    "- Why CoT improves factuality and reasoning\n",
    "- Example: CoT prompt for multi-hop question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70026ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Chain-of-thought prompt\n",
    "cot_prompt = \"\"\"Q: If John is in Paris and Paris is in France, where is John?\\nA: Let's think step by step. John is in Paris. Paris is in France. So John is in France.\"\"\"\n",
    "print(cot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c015928",
   "metadata": {},
   "source": [
    "## 7. Memory Systems & Context Management <a id='memory-systems'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13452fc3",
   "metadata": {},
   "source": [
    "### Memory Systems & Context Management\n",
    "Memory systems help LLMs maintain context over long conversations or documents.\n",
    "\n",
    "- Types: short-term, long-term, vector store memory\n",
    "- Context window management, summarization, retrieval\n",
    "- Example: Using ChromaDB for conversational memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b1ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Store and retrieve conversation history with ChromaDB\n",
    "import chromadb\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(\"chat_memory\")\n",
    "collection.add(documents=[\"Hello, how can I help you?\", \"What is the weather today?\"], metadatas=[{\"role\": \"assistant\"}, {\"role\": \"user\"}], ids=[\"1\", \"2\"])\n",
    "results = collection.query(query_texts=[\"weather\"], n_results=1)\n",
    "print(\"Relevant memory:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55003bab",
   "metadata": {},
   "source": [
    "## 8. Hands-on Project <a id='hands-on-project'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b6eeca",
   "metadata": {},
   "source": [
    "### Hands-on Project: Multi-modal RAG System\n",
    "- Build a RAG system that retrieves both text and images\n",
    "- Integrate a simple knowledge graph for context expansion\n",
    "- Use advanced prompt engineering and chain-of-thought reasoning\n",
    "- Implement conversational memory with a vector database\n",
    "- Evaluate retrieval and generation quality"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
