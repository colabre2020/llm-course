{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57346054",
   "metadata": {},
   "source": [
    "# Week 6: LLM Fine-tuning & Optimization\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand parameter-efficient fine-tuning methods (LoRA, QLoRA)\n",
    "- Apply distributed training and model parallelism\n",
    "- Explore quantization, pruning, and knowledge distillation\n",
    "- Optimize LLMs for cost, speed, and memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672e9486",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction to LLM Fine-tuning](#introduction)\n",
    "2. [LoRA and QLoRA](#lora-qlora)\n",
    "3. [Parameter-Efficient Fine-tuning](#peft)\n",
    "4. [Distributed Training & Model Parallelism](#distributed-training)\n",
    "5. [Quantization & Pruning](#quantization-pruning)\n",
    "6. [Knowledge Distillation](#knowledge-distillation)\n",
    "7. [Optimization for Production](#optimization)\n",
    "8. [Hands-on Project](#hands-on-project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f20009",
   "metadata": {},
   "source": [
    "## 1. Introduction to LLM Fine-tuning <a id='introduction'></a>\n",
    "Fine-tuning adapts a pre-trained LLM to a specific task or domain. Modern techniques focus on efficiency, scalability, and cost.\n",
    "\n",
    "- Why fine-tune? Customization, performance, domain adaptation\n",
    "- Challenges: compute, data, overfitting, catastrophic forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2217e9c",
   "metadata": {},
   "source": [
    "## 2. LoRA and QLoRA <a id='lora-qlora'></a>\n",
    "LoRA (Low-Rank Adaptation) and QLoRA (Quantized LoRA) are parameter-efficient fine-tuning methods.\n",
    "\n",
    "- LoRA: Injects trainable low-rank matrices into transformer layers\n",
    "- QLoRA: Combines LoRA with quantization for memory efficiency\n",
    "- Example: Fine-tuning with LoRA using Hugging Face PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981e50a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Fine-tuning with LoRA (Hugging Face PEFT)\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "lora_config = LoraConfig(task_type=TaskType.CAUSAL_LM, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d01a4bd",
   "metadata": {},
   "source": [
    "## 3. Parameter-Efficient Fine-tuning <a id='peft'></a>\n",
    "PEFT methods reduce the number of trainable parameters, making fine-tuning feasible on smaller hardware.\n",
    "\n",
    "- Adapters, prefix-tuning, prompt-tuning\n",
    "- Trade-offs: flexibility vs. efficiency\n",
    "- Example: Prefix-tuning with Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4431e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Prefix-tuning (conceptual)\n",
    "from peft import PrefixTuningConfig\n",
    "prefix_config = PrefixTuningConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=10)\n",
    "# model = get_peft_model(model, prefix_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b89f017",
   "metadata": {},
   "source": [
    "## 4. Distributed Training & Model Parallelism <a id='distributed-training'></a>\n",
    "Large models require distributed training across multiple GPUs or nodes.\n",
    "\n",
    "- Data parallelism, model parallelism, pipeline parallelism\n",
    "- Tools: DeepSpeed, Hugging Face Accelerate, PyTorch DDP\n",
    "- Example: Launching distributed training with Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d38642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Launch distributed training (conceptual)\n",
    "# accelerate launch train.py --config_file=accelerate_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c06a99",
   "metadata": {},
   "source": [
    "## 5. Quantization & Pruning <a id='quantization-pruning'></a>\n",
    "Quantization reduces model size and inference cost by lowering precision. Pruning removes redundant weights.\n",
    "\n",
    "- Post-training quantization, quantization-aware training\n",
    "- Structured and unstructured pruning\n",
    "- Example: Quantize a model with Hugging Face Optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c4b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Quantize a model (conceptual)\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "model = OVModelForCausalLM.from_pretrained(\"gpt2\", export=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3740fa12",
   "metadata": {},
   "source": [
    "## 6. Knowledge Distillation <a id='knowledge-distillation'></a>\n",
    "Distillation transfers knowledge from a large teacher model to a smaller student model.\n",
    "\n",
    "- Teacher-student paradigm\n",
    "- Loss functions: soft targets, hard targets\n",
    "- Example: Distillation with Hugging Face Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042302c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Distillation (conceptual)\n",
    "# from transformers import Trainer, TrainingArguments\n",
    "# trainer = Trainer(model=student, args=TrainingArguments(...), teacher_model=teacher)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcc7db5",
   "metadata": {},
   "source": [
    "## 7. Optimization for Production <a id='optimization'></a>\n",
    "Optimizing LLMs for production involves balancing speed, cost, and accuracy.\n",
    "\n",
    "- Model compilation (ONNX, TorchScript)\n",
    "- Batch inference, request batching\n",
    "- Monitoring and profiling\n",
    "- Example: Export to TorchScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e34371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Export model to TorchScript\n",
    "import torch\n",
    "traced = torch.jit.trace(model, torch.zeros(1, 8, dtype=torch.long))\n",
    "traced.save(\"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b82605",
   "metadata": {},
   "source": [
    "## 8. Hands-on Project <a id='hands-on-project'></a>\n",
    "\n",
    "### Hands-on Project: Fine-tuning and Optimizing a Small LLM\n",
    "\n",
    "**Goal:** Fine-tune a small LLM (e.g., DistilGPT2, TinyLlama) using LoRA or QLoRA on a custom dataset, optimize it for efficient inference, deploy it, and benchmark its performance.\n",
    "\n",
    "#### Step 1: Prepare Your Custom Dataset\n",
    "- Choose a domain (e.g., customer support, finance, healthcare, etc.)\n",
    "- Format your dataset for language modeling or instruction tuning (CSV, JSON, or Hugging Face Datasets format)\n",
    "- Split into train/validation sets\n",
    "- Example: [Hugging Face Datasets Quickstart](https://huggingface.co/docs/datasets/quickstart)\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "# Example: Load a public dataset (replace with your own for custom)\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "train_data = dataset[\"train\"]\n",
    "val_data = dataset[\"test\"]\n",
    "```\n",
    "\n",
    "#### Step 2: Fine-tune with LoRA or QLoRA\n",
    "- Use Hugging Face PEFT or similar libraries\n",
    "- Configure LoRA/QLoRA parameters for your hardware\n",
    "- Train and save the adapted model\n",
    "- Example: [PEFT LoRA Quickstart](https://huggingface.co/docs/peft/task_guides/peft_lora)\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "lora_config = LoraConfig(task_type=TaskType.CAUSAL_LM, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Tokenize your dataset and set up Trainer as usual\n",
    "# ...\n",
    "# trainer = Trainer(model=model, ...)\n",
    "# trainer.train()\n",
    "```\n",
    "\n",
    "#### Step 3: Apply Quantization or Distillation\n",
    "- Quantize the model for faster, cheaper inference (e.g., 8-bit, 4-bit)\n",
    "- Optionally, distill knowledge from a larger teacher model\n",
    "- Save the optimized model\n",
    "- Example: [Hugging Face Optimum Quantization](https://huggingface.co/docs/optimum/intel/usage_guides/quantization)\n",
    "\n",
    "```python\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "model = OVModelForCausalLM.from_pretrained(\"distilgpt2\", export=True)\n",
    "```\n",
    "\n",
    "#### Step 4: Deploy and Benchmark\n",
    "- Serve the model using FastAPI, Flask, or another framework\n",
    "- Containerize with Docker for portability\n",
    "- Benchmark latency, throughput, and memory usage\n",
    "- Compare performance before and after optimization\n",
    "- Example: [Hugging Face Inference Endpoints](https://huggingface.co/inference-endpoints)\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from transformers import pipeline\n",
    "app = FastAPI()\n",
    "pipe = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "@app.post(\"/generate\")\n",
    "def generate(prompt: str):\n",
    "    return pipe(prompt)\n",
    "```\n",
    "\n",
    "#### Step 5: Document Your Workflow\n",
    "- Summarize your process, challenges, and results\n",
    "- Include code snippets, metrics, and lessons learned\n",
    "- Suggest improvements for future projects\n",
    "\n",
    "---\n",
    "\n",
    "**References & Further Reading:**\n",
    "- [Hugging Face PEFT Documentation](https://huggingface.co/docs/peft/index)\n",
    "- [Hugging Face Transformers Fine-tuning Guide](https://huggingface.co/docs/transformers/training)\n",
    "- [Hugging Face Optimum (Quantization)](https://huggingface.co/docs/optimum/intel/usage_guides/quantization)\n",
    "- [Deploying with FastAPI](https://fastapi.tiangolo.com/tutorial/first-steps/)\n",
    "- [Hugging Face Inference Endpoints](https://huggingface.co/inference-endpoints)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
