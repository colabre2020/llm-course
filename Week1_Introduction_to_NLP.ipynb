{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fa74309",
   "metadata": {},
   "source": [
    "# Week 1: Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the fundamentals of Natural Language Processing\n",
    "- Learn about tokenization, embeddings, and vector representations\n",
    "- Explore text preprocessing techniques\n",
    "- Introduction to language models\n",
    "\n",
    "## Table of Contents\n",
    "1. [What is NLP?](#what-is-nlp)\n",
    "2. [Text Preprocessing](#text-preprocessing)\n",
    "3. [Tokenization](#tokenization)\n",
    "4. [Word Embeddings](#word-embeddings)\n",
    "5. [Vector Representations](#vector-representations)\n",
    "6. [Basic Language Models](#basic-language-models)\n",
    "7. [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50879fa",
   "metadata": {},
   "source": [
    "## What is NLP?\n",
    "\n",
    "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language. It involves developing algorithms and models that can understand, interpret, and generate human language in a valuable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37707ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install nltk spacy transformers torch numpy pandas matplotlib seaborn\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e401d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b014619",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "Text preprocessing is a crucial step in NLP that involves cleaning and preparing raw text data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd967273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Basic text preprocessing function\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example\n",
    "sample_text = \"Hello World! This is a sample text with numbers 123 and symbols @#$.\"\n",
    "preprocessed = preprocess_text(sample_text)\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Preprocessed: {preprocessed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c8caf6",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of breaking down text into individual tokens (words, subwords, or characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f8daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural Language Processing is fascinating. It helps computers understand human language.\"\n",
    "\n",
    "# Word tokenization\n",
    "words = word_tokenize(text)\n",
    "print(\"Word tokens:\", words)\n",
    "\n",
    "# Sentence tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"\\nSentence tokens:\", sentences)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words and word.isalpha()]\n",
    "print(\"\\nFiltered words:\", filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1a2eac",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "Word embeddings are dense vector representations of words that capture semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cefe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using spaCy for word embeddings\n",
    "doc = nlp(\"king queen man woman\")\n",
    "\n",
    "# Get word vectors\n",
    "for token in doc:\n",
    "    print(f\"Word: {token.text}\")\n",
    "    print(f\"Vector shape: {token.vector.shape}\")\n",
    "    print(f\"First 5 dimensions: {token.vector[:5]}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb27300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity between words\n",
    "def calculate_similarity(word1, word2):\n",
    "    token1 = nlp(word1)[0]\n",
    "    token2 = nlp(word2)[0]\n",
    "    return token1.similarity(token2)\n",
    "\n",
    "# Examples\n",
    "print(f\"Similarity between 'king' and 'queen': {calculate_similarity('king', 'queen'):.3f}\")\n",
    "print(f\"Similarity between 'king' and 'car': {calculate_similarity('king', 'car'):.3f}\")\n",
    "print(f\"Similarity between 'happy' and 'joyful': {calculate_similarity('happy', 'joyful'):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e1bb23",
   "metadata": {},
   "source": [
    "## Vector Representations\n",
    "\n",
    "Let's explore different ways to represent text as vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d84991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"I love natural language processing\",\n",
    "    \"Machine learning is fascinating\",\n",
    "    \"Deep learning models are powerful\",\n",
    "    \"Natural language understanding is important\"\n",
    "]\n",
    "\n",
    "# Bag of Words (Count Vectorizer)\n",
    "count_vectorizer = CountVectorizer()\n",
    "bow_matrix = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Bag of Words representation:\")\n",
    "print(\"Features:\", count_vectorizer.get_feature_names_out())\n",
    "print(\"Matrix shape:\", bow_matrix.shape)\n",
    "print(\"Matrix:\\n\", bow_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc32b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"\\nTF-IDF representation:\")\n",
    "print(\"Features:\", tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"Matrix shape:\", tfidf_matrix.shape)\n",
    "print(\"Matrix:\\n\", tfidf_matrix.toarray().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276f0437",
   "metadata": {},
   "source": [
    "## Basic Language Models\n",
    "\n",
    "Introduction to n-gram language models and their limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ec14da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "class SimpleNGramModel:\n",
    "    def __init__(self, n=2):\n",
    "        self.n = n\n",
    "        self.ngrams = defaultdict(list)\n",
    "    \n",
    "    def train(self, text):\n",
    "        words = text.lower().split()\n",
    "        for i in range(len(words) - self.n + 1):\n",
    "            context = ' '.join(words[i:i+self.n-1])\n",
    "            next_word = words[i+self.n-1]\n",
    "            self.ngrams[context].append(next_word)\n",
    "    \n",
    "    def predict_next(self, context):\n",
    "        if context in self.ngrams:\n",
    "            return random.choice(self.ngrams[context])\n",
    "        return None\n",
    "    \n",
    "    def generate(self, start_text, length=10):\n",
    "        words = start_text.lower().split()\n",
    "        for _ in range(length):\n",
    "            context = ' '.join(words[-(self.n-1):])\n",
    "            next_word = self.predict_next(context)\n",
    "            if next_word:\n",
    "                words.append(next_word)\n",
    "            else:\n",
    "                break\n",
    "        return ' '.join(words)\n",
    "\n",
    "# Example usage\n",
    "training_text = \"natural language processing is a field of artificial intelligence that focuses on the interaction between computers and human language\"\n",
    "\n",
    "model = SimpleNGramModel(n=2)\n",
    "model.train(training_text)\n",
    "\n",
    "generated_text = model.generate(\"natural language\", length=8)\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd72deec",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Text Analysis\n",
    "Analyze a piece of text and extract basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c23eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(text):\n",
    "    \"\"\"Analyze text and return basic statistics\"\"\"\n",
    "    # Your code here\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Basic statistics\n",
    "    num_characters = len(text)\n",
    "    num_words = len([token for token in doc if token.is_alpha])\n",
    "    num_sentences = len(list(doc.sents))\n",
    "    num_unique_words = len(set([token.text.lower() for token in doc if token.is_alpha]))\n",
    "    \n",
    "    # Part-of-speech tags\n",
    "    pos_tags = [token.pos_ for token in doc if token.is_alpha]\n",
    "    pos_counts = Counter(pos_tags)\n",
    "    \n",
    "    # Named entities\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    \n",
    "    return {\n",
    "        'num_characters': num_characters,\n",
    "        'num_words': num_words,\n",
    "        'num_sentences': num_sentences,\n",
    "        'num_unique_words': num_unique_words,\n",
    "        'lexical_diversity': num_unique_words / num_words if num_words > 0 else 0,\n",
    "        'pos_counts': dict(pos_counts),\n",
    "        'entities': entities\n",
    "    }\n",
    "\n",
    "# Test with sample text\n",
    "sample = \"Apple Inc. is an American multinational technology company. It was founded by Steve Jobs in California.\"\n",
    "analysis = analyze_text(sample)\n",
    "print(\"Text Analysis Results:\")\n",
    "for key, value in analysis.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def79ca2",
   "metadata": {},
   "source": [
    "### Exercise 2: Build a Simple Text Classifier\n",
    "Create a basic sentiment classifier using traditional ML approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c145b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Sample sentiment data\n",
    "texts = [\n",
    "    \"I love this movie\", \"This is amazing\", \"Great product\", \"Excellent service\",\n",
    "    \"I hate this\", \"This is terrible\", \"Bad quality\", \"Worst experience ever\",\n",
    "    \"It's okay\", \"Not bad\", \"Average product\", \"Could be better\"\n",
    "]\n",
    "\n",
    "labels = [1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 2, 2]  # 1: positive, 0: negative, 2: neutral\n",
    "\n",
    "# Create features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "y = labels\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive', 'Neutral']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f13fa33",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this module, we covered:\n",
    "- Fundamentals of NLP\n",
    "- Text preprocessing techniques\n",
    "- Tokenization methods\n",
    "- Word embeddings and vector representations\n",
    "- Basic language models\n",
    "- Simple text classification\n",
    "\n",
    "## Next Steps\n",
    "In the next module, we'll explore Transformers and LLM system design, building upon these foundational concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5107f8",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "- [NLTK Documentation](https://www.nltk.org/)\n",
    "- [spaCy Documentation](https://spacy.io/)\n",
    "- [Scikit-learn Text Processing](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "- [Word2Vec Paper](https://arxiv.org/abs/1301.3781)\n",
    "- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
